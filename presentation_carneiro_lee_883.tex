% presentation_carneiro_lee_883.tex
% 20-30 minute presentation on Carneiro and Lee (2009)
% Jackson Bunting; Drew Vollmer 03-25-2017

\documentclass{beamer}

\usepackage{amsfonts} % For \mathbb command
\usepackage{amsmath} % for align
\usepackage{graphicx} % for graphics
\usepackage{cancel} % for ...


\allowdisplaybreaks % so that aligned equations can be broken across pages
\newcommand{\E}{\mathrm{E}} % Expectation operator
\newcommand{\Var}{\mathrm{Var}} % Variance operator
\mathchardef\mhyphen="2D % Create a hyphen for math mode
\newcommand*\diff{\mathop{}\!d} % nicely formatted integral dx

\usetheme{madrid}

\begin{document}



\title[Distributions of Potential Outcomes]{Estimating Distributions
  of Potential Outcomes Using Local Instrumental Variables}
\subtitle{Carneiro and Lee, Journal of Econometrics (2009)}
\author[]{Jackson Bunting and Drew Vollmer}
\frame{\maketitle}


\begin{frame}{Overview}

% Nontechnical overview of big ideas
\begin{itemize}

\item Context: we know that the marginal treatment effect (MTE) can be
  used to construct other treatment effects of interest
\begin{itemize}
\item MTE can be estimated using local instrumental variables
\end{itemize}

\pause

\item Problem: simple treatment effects, like $\E(Y_1 - Y_0 | \dots)$,
  might be too crude to properly assess policies
\begin{itemize}
\item Consider returns to college, which vary across the population

\item Policy evaluation should care about the marginal enrollee, but
  the marginal effect depends on who is already enrolled
\end{itemize}

\pause

\item Satisfying answers require a way to estimate the distribution of
  changes in potential outcomes
\begin{itemize}
\item This is the goal of Carneiro and Lee's paper
\end{itemize}

\end{itemize}

\end{frame}




\begin{frame}{Econometric Model} % Or framework, whichever is appropriate

\begin{itemize}

\item Setting is almost identical to the one for MTEs

\item $Y_i = \mu_i(X, U_i)$ for $i \in \{0, 1\}$

\item Selection follows $D = \mathbb{I}[ \mu_D(Z) \geq U_D ]$

\end{itemize}

\pause

\begin{enumerate}

\item $\mu_D(Z)$ is nondegenerate conditional on $X$

\item $(U_i, U_D) \perp Z | X$

\item Distribution of $U_D$ is abs. continuous $\dots$

\item Normalize unobservables so $U_i | X, Z \sim U[0, 1]$

\pause

\item $\E | G(Y_i) | < \infty$

\end{enumerate}

\end{frame}


% Context for theorem 1: essential background on the MTE and local
% instrumental variables to put the result in context
\begin{frame}{Identification Context}

\begin{itemize}

% How does P(Z) connect to our notation?
\item Local IV result:

\begin{align*}
  \frac{\partial \E \left( Y | X = x, P(Z) = p \right) }{\partial p} &= \Delta^{MTE}(x, p) \\
  &= \E \left( Y_1 - Y_0 | X = x, U_d = u_d \right)
\end{align*}

% Comments on implications and estimation
\pause

\item Carneiro and Lee's contribution: generalize estimation of the
  MTE to an arbitrary function $G(Y)$

\end{itemize}

\end{frame}





% Shrink to make the math type just a bit smaller
\begin{frame}[shrink = 1]{Key Identification Result}

\begin{theorem}
  Under the potential outcomes framework, selection equation, and
  technical assumptions,

\vspace{-.25cm}
\begin{align*}
  \E\left[ G(Y_1) | X = x, U_D = p \right] = &\E \left[ G(Y) | X = x, \mu_D(Z) = p, D = 1 \right] \\
  &+ p \frac{\partial \E\left[ G(Y) | X = x, \mu_D(Z) = p, S = 1 \right]}{\partial p} \\
  \E\left[ G(Y_0) | X = x, U_D = p \right] = &\E \left[ G(Y) | X = x, \mu_D(Z) = p, D = 0 \right] \\
  &- (1 - p) \frac{\partial \E\left[ G(Y) | X = x, \mu_D(Z) = p, S = 0 \right]}{\partial p} \\
\end{align*}
\end{theorem}

\end{frame}


% Need text a bit smaller, so use the shrink parameter
% Tutorial on using action and others in beamer: https://www.sharelatex.com/blog/2013/08/20/beamer-series-pt4.html
\begin{frame}[shrink = 10]{Proof of Theorem}

\begin{proof}
\[
\begin{aligned}
\action<1->{\E [ G(Y) | &X = x, \mu_D(Z) = p, D = 1 ] = \E \left[ G(Y) | X = x, \mu_D(Z) = p, U_D < p \right] \\}
\action<1->{&= \E \left[ G(Y_1) | X = x, \mu_D(Z) = p, U_D < p \right] = \E \left[ G(Y_1) | X = x, U_D < p \right] \\}
\action<2->{&= \frac{1}{\Pr(U_D < p)} \int_0^p \E \left[ G(Y_1) | X = x, U_D = u_D \right] f_{U_D | X}(u_D | x) \diff u_D \\}
\action<3->{&= \frac{1}{p} \int_0^p \E \left[ G(Y_1) | X = x, U_D = u_D \right] \int \underbrace{f_{U_D | X, Z}(u_D | x, z)}_{1} f_{Z|X}(z | x) \diff z \diff u_D \\}
\action<3->{&= \frac{1}{p} \int_0^p \E \left[ G(Y_1) | X = x, U_D = u_D \right] \diff u_D \\}
\end{aligned}
\]

\action<3->{Conclude by differentiating both sides with respect to $p$.}

\end{proof}
\end{frame}


% Explain why the theorem matters
\begin{frame}{Implications}
% So far we have: (1) can get distributions 
\end{frame}


% Construction of treatment effects from theorem 1
\begin{frame}{Getting Treatment Effects}
\end{frame}

\begin{frame}{Estimation} 
Observe $\{Y,S,X,Z\}$ iid. Two objects of interest :
\begin{enumerate}
\item Marginal potential outcomes---difference is MTE
\begin{equation*}
  \mathrm{E}[Y_1\mid x,U_d=p]
\end{equation*}
\item Distribution of marginal potential outcomes
  \begin{equation*}
    f_{Y_1\mid X,U_d}(y\mid x,U_d=p)
  \end{equation*}
\end{enumerate}
\begin{itemize}
\item The second builds on the first
\item Empirical section is application of suggested estimators [I will
  delete this, since it should be in overview]
\end{itemize}
  
\end{frame}

\begin{frame}{Estimation: Marginal Potential Outcomes  }
  \begin{itemize}
  \item Authors assume structure for $\mu_1$:
    \begin{equation*}
      Y_1  = X'\beta_1 +  U_1
    \end{equation*}
\item So we wish to estimate:
    \begin{equation*}
      \mathrm{E}[Y_1\mid x,U_d=p]  = x'\beta_1 +  \mathrm{E}[U_1\mid x,U_d=p]
    \end{equation*} 
\item The two terms are estimated separately
  \end{itemize}
\end{frame}

\begin{frame}{Estimation:  Marginal Potential Outcomes, $\beta_1$}
  \begin{itemize}
\item By the structural form:
\begin{equation*}
  \mathrm{E}[Y_1\mid x, p,S=1] = x'\beta_1 +  \mathrm{E}[U_1\mid x,p,
  S=1   ]
\end{equation*}
\item It can be shown $\mathrm{E}[U_1\mid x,p,
  S=1   ]$ depends only on $p$. Say: 
  \begin{equation*}
    \mathrm{E}[Y_1\mid x, p,S=1] = x'\beta_1 +  \lambda_1(p)
  \end{equation*}
%%%[The idea is that (z,x) drop out by independence, then Ud drops out
%%%because its uniform.][Also ind of x not necessary]
\item If define mean zero errors $\epsilon_i$, this can be written as:
 \begin{equation*}
    Y_1 = x'\beta_1 +  \lambda_1(p) + \epsilon
  \end{equation*}
%%%[Y=Y1 since we are among S=1]
\item This is a `Partial Linear Model'! (The terms we care about are linear)
\end{itemize}
\end{frame}

\begin{frame}{Estimation:  Marginal Potential Outcomes, $\beta_1$}
  \begin{itemize}
  \item Robinson (1988) has a estimator for this model
\item The idea is to
  `difference' away $\lambda(p)$
%%%like in panel data:
  \begin{equation*}
Y_1 - E[Y_1\mid p] = (x'-E[x\mid p])\beta_1 +  \cancel{\lambda_1(p) - \lambda_1(p)} + \eta
\end{equation*}
\item The estimation involves:
\begin{enumerate}
\item Estimate $\hat{P}$ non-parametrically 
%%%[use siepe regression: a power series approximation]
\item Regress $Y$ on $\hat{p}$ and $X$ on $\hat{p}$, store the fitted values
\item Then do OLS of $Y - E[Y\mid p]$ on $X -
   E[X\mid p]$
\end{enumerate}
  \end{itemize}
  
\end{frame}

\begin{frame}{Estimation:  Marginal Potential Outcomes }
  \begin{itemize}
  \item Now for the second bit: \begin{equation*}
      \mathrm{E}[Y_1\mid x,U_d=p]  = x'\beta_1 +  \mathbf{\mathrm{\mathbf{E}}[U_1\mid x,U_d=p]}
    \end{equation*}
\item Our sample is $\{\hat{U}_1= Y-\mu_1(x,\hat{\beta}_1), \hat{P},
  S\}$
\item The ID argument replicates for $U_1$, so 
\begin{equation*}
  \mathrm{E}[U_1\mid U_d= p] = p \frac{\partial}{\partial
    p}\mathrm{E}[U_1\mid P=p,S=1] + \mathrm{E}[U_1\mid P=p,S=1]
\end{equation*}
\item We can estimate the RHS with local polynomial regression
  \end{itemize}

\end{frame}

\begin{frame}{Estimation:  Marginal Potential Outcomes,
    $\mathrm{E}[U_1\mid U_d=p]$}
    \begin{itemize}
  \item LP regression is like LS weighting obserations close to $\hat{P}=p$
%%% [``locally'']
\item For example, solve
  \begin{equation*}
   \min_{\gamma_1,\gamma_2} \left\{ \sum_i\left[\hat{U}_{1i}-(\gamma_1 +
      \gamma_2(\hat{P}_i-p))\right]^2\mathrm{1}{\bigl(|\hat{P}_i - p| \le h\bigr)}\right\}
  \end{equation*}
%%% [I've told you what it is, now I'll give you motivation]
\item Why? Think of a taylor series of $E[\hat{U}_{1}|
 \hat{P}=\hat{P}_i]$
  about $\hat{P} = p$
  \begin{equation*}
    E[\hat{U}_{1}\mid \hat{P}_i] \approx E[\hat{U}_{1}\mid p] +
    \frac{\partial}{\partial p} E[\hat{U}_{1}\mid p] (\hat{P}_i-p)
  \end{equation*}
With equality as $p \rightarrow \hat{P}_i$. This justifies
interpretation that
\begin{eqnarray*}
  \hat{\gamma}_1 = \hat{E}[\hat{U}_{1}\mid p] & & \hat{\gamma}_2 = \frac{\partial}{\partial p} \hat{E}[\hat{U}_{1}\mid p]
\end{eqnarray*}
  \end{itemize}
\end{frame}

\begin{frame}{Recap:  Marginal Potential Outcomes}
\begin{enumerate}
\item Marginal potential outcomes---difference is MTE
\begin{equation*}
  \mathrm{E}[Y_1\mid x,U_d=p] = x'\beta_1 +
  \mathrm{E}[U_1\mid x,U_d=p]
\end{equation*}
\item Distribution of marginal potential outcomes
  \begin{equation*}
    f_{Y_1\mid X,U_d}(y\mid x,p)
  \end{equation*}
\end{enumerate}

We did \#1 by
\begin{itemize}
\item Imposing linearity and separability
\item Recovering $\hat{P}$ using sieves
\item Recovering $\hat{\beta}$ using Robinson's partial linear
  estimator
\item Estimating the selection error by local polynomial regression
\end{itemize}
Now \#2...
  
\end{frame}

\begin{frame}{Estimation: $f_{Y_1\mid X,U_d}(y\mid x,p)$}
  \begin{itemize}
\item Authors assume $X$ is independent of $U$, so 
\begin{equation*}
  f_{y_.|x,p}(y) = f_{u_.|p}(y-\mu_.(x,\beta_.))
\end{equation*}
%%% I could write this on the board if unclear

\item From ID result:
  \begin{equation*}
    f_{u_.|p}(u) = p \frac{\partial}{\partial
    p}f_{u_.|p,s=1}(u) + f_{u_.|p,s=1}(u)
  \end{equation*}
%%% If discrete use indicator, if cont use CDF and differentiate
\item There is also a result that says kernel density estimation is
  unbiased:
  \begin{equation*}
    \mathrm{E}\left[h^{-1}K\left(\frac{U_1-u}{h}\right)\mid P=p,
      S=1\right]\xrightarrow[h \rightarrow 0]{} f_{U_1|P,S=1}(u\mid p)
  \end{equation*}
%%% Says that 
\item So suggests to get density, we could estimate the conditional mean of
  $h^{-1}K(.)$ locally (among those with $P=p$) among the treated 
  \end{itemize}
 $\rightarrow$ This is local polynomial regression again

  
\end{frame}



\end{document}
