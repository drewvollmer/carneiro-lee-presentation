% presentation_carneiro_lee_883.tex
% 20-30 minute presentation on Carneiro and Lee (2009)
% Jackson Bunting; Drew Vollmer 03-25-2017

\documentclass{beamer}

\usepackage{amsfonts} % For \mathbb command
\usepackage{amsmath} % for align
\usepackage{graphicx} % for graphics
\usepackage{cancel} % for ...


\allowdisplaybreaks % so that aligned equations can be broken across pages
\newcommand{\E}{\mathrm{E}} % Expectation operator
\newcommand{\Var}{\mathrm{Var}} % Variance operator
\mathchardef\mhyphen="2D % Create a hyphen for math mode
\newcommand*\diff{\mathop{}\!d} % nicely formatted integral dx

\usetheme{Madrid}

\begin{document}



\title[Distributions of Potential Outcomes]{Estimating Distributions
  of Potential Outcomes Using Local Instrumental Variables}
\subtitle{Carneiro and Lee, Journal of Econometrics (2009)}
\author[]{Jackson Bunting and Drew Vollmer}
\frame{\maketitle}


\begin{frame}{Overview}

% Nontechnical overview of big ideas
\begin{itemize}

\item Context: we know that the marginal treatment effect (MTE) can be
  used to construct other treatment effects of interest
\begin{itemize}
\item MTE can be estimated using local instrumental variables
\end{itemize}

\pause

\item Problem: simple treatment effects, like $\E(Y_1 - Y_0 | \dots)$,
  might be too crude to properly assess policies
\begin{itemize}
\item Policy design depends on marginal, not average, effects
\end{itemize}

\pause

\item It would be useful to estimate the expectation and distribution
  of potential outcomes $Y_0$ and $Y_1$, not just $Y_1 - Y_0$
\begin{itemize}
\item This is the goal of Carneiro and Lee's paper
\end{itemize}

\end{itemize}

\end{frame}


\begin{frame}{Overview, continued}

\begin{enumerate}

\item Identify marginal potential outcomes separately using a strategy
  similar to local IV

\item Use the identification result to propose estimators for marginal
  potential outcomes and their distributions

\item Discuss asymptotic properties of the estimators

\item Empirical application: returns to college

\end{enumerate}

\end{frame}




\begin{frame}{Econometric Model} % Or framework, whichever is appropriate

\begin{itemize}

\item Setting is almost identical to the one for MTEs

\item $Y_i = \mu_i(X, U_i)$ for $i \in \{0, 1\}$

\item Selection follows $D = \mathbb{I}[ \mu_D(Z) \geq U_D ]$

\end{itemize}

\pause

\begin{enumerate}

\item $\mu_D(Z)$ is nondegenerate conditional on $X$

\item $(U_i, U_D) \perp Z | X$

\item Distribution of $U_D$ is abs. continuous $\dots$

\item Normalize unobservables so $U_i | X, Z \sim U[0, 1]$

\item $\E|Y_i| < \infty$ generalized to $\E | G(Y_i) | < \infty$

\end{enumerate}

\pause

\begin{itemize}
\item Assumptions are no stronger than those from class: only need a
  valid instrument and a selection model, plus technical conditions
\begin{itemize}
\item To estimate, we'll need stronger assumptions
\end{itemize}
\end{itemize}


\end{frame}


% Context for theorem 1: essential background on the MTE and local
% instrumental variables to put the result in context
\begin{frame}{Identification Context}

\begin{itemize}

% P(Z) is the propensity score \mu_D(Z)
\item Local IV result:

\begin{align*}
  \frac{\partial \E \left( Y | X = x, \mu_D(Z) = p \right) }{\partial p} &= \Delta^{MTE}(x, p) \\
  &= \E \left( Y_1 - Y_0 | X = x, U_d = u_d \right)
\end{align*}


\item Need a continuous instrument to get variation in the propensity
  score
\begin{itemize}
\item Can only learn about the MTE on the support of the propensity
  score
\end{itemize}


% Comments on implications and estimation
\pause

\item Local IV only tells us about the difference in potential
  outcomes
\begin{itemize}
\item It would be valuable to learn about $Y_0$ and $Y_1$ separately
\end{itemize}

\item Carneiro and Lee's contribution: generalize estimation of the
  MTE to an arbitrary function $G(Y)$

\end{itemize}

\end{frame}





% Shrink to make the math type just a bit smaller
\begin{frame}[shrink = 1]{Key Identification Result}

\begin{theorem}
  Under the potential outcomes framework, selection equation, and
  technical assumptions,

\vspace{-.25cm}
\begin{align*}
  \E\left[ G(Y_1) | X = x, U_D = p \right] = &\E \left[ G(Y) | X = x, \mu_D(Z) = p, D = 1 \right] \\
  &+ p \frac{\partial \E\left[ G(Y) | X = x, \mu_D(Z) = p, S = 1 \right]}{\partial p} \\
  \E\left[ G(Y_0) | X = x, U_D = p \right] = &\E \left[ G(Y) | X = x, \mu_D(Z) = p, D = 0 \right] \\
  &- (1 - p) \frac{\partial \E\left[ G(Y) | X = x, \mu_D(Z) = p, S = 0 \right]}{\partial p} \\
\end{align*}
\end{theorem}

\end{frame}


% Need text a bit smaller, so use the shrink parameter
% Tutorial on using action and others in beamer: https://www.sharelatex.com/blog/2013/08/20/beamer-series-pt4.html
\begin{frame}{Proof of Theorem}
\fontsize{10pt}{7.2}\selectfont
\begin{proof}
\[
\begin{aligned}
\action<1->{\E [ G(Y) | &X = x, \mu_D(Z) = p, D = 1 ] = \E \left[ G(Y) | X = x, \mu_D(Z) = p, U_D < p \right] \\}
\action<1->{&= \E \left[ G(Y_1) | X = x, \alert<1>{\mu_D(Z) = p}, U_D < p \right] = \E \left[ G(Y_1) | X = x, U_D < p \right] \\}
\action<1->{&\only<1>{\alert<1>{\mathrm{Removed\ because\ } (Y_0, Y_1) \perp Z|X.}}
             \only<2->{= \frac{1}{\Pr(U_D < p)} \int_0^p \E \left[ G(Y_1) | X = x, U_D = u_D \right] \alert<2>{f_{U_D | X}(u_D | x)} \diff u_D} \\}
\action<2->{&\only<2>{\alert<2>{f_{U_D|X}(u_D|x) = \E\left( f_{U_D|X, Z}(u_D | X = x, Z = z) | X = x \right) = \E(1 | X = x) = 1}}
            \only<3->{= \frac{1}{p} \int_0^p \E \left[ G(Y_1) | X = x, U_D = u_D \right] \diff u_D } \\}
\end{aligned}
\]

\vspace{-.5cm}
\action<4->{Next, differentiate with respect to $p$.}
\vspace{-.25cm}
\[
\begin{aligned}
\action<5->{\frac{\partial}{\partial p} \int_0^p \E \left[ G(Y_1) | X = x, U_D = u_D \right] \diff u_D &= \frac{\partial}{\partial p} p \E \left[ G(Y) | X = x, \mu_D(Z) = p, D = 1 \right] \\}
\action<6->{\E \left[ G(Y_1) | X = x, U_D = p \right] &= \E \left[ G(Y) | X = x, \mu_D(Z) = p, D = 1 \right] \\}
\action<6->{&+ p \frac{\partial \E \left[ G(Y) | X = x, \mu_D(Z) = p, D = 1 \right]}{\partial p} \\}
\end{aligned}
\]

\vspace{-.75cm}
\action<6->{Proof for $Y_0$ is analogous.}
\end{proof}
\end{frame}


% Explain why the theorem matters
\begin{frame}{Implications}

\begin{itemize}

\item Under general conditions, moments of potential outcomes are
  identified
\begin{itemize}
\item $G(y) = y$ provides the MTE

\item $G(y) = \mathbb{I} \left[ Y \leq y \right]$ provides the
  distribution
\end{itemize}

\pause

\item From the MTE, recover all standard treatment effects

\item From the CDF, recover the density of potential outcomes
% Quantile treatment effects?

\pause

\item The theorem also applies to unobservables
\begin{itemize}
\item The distribution of unobservables is also identified
\end{itemize}

\end{itemize}

\end{frame}


\begin{frame}{From identification to estimation}
  \begin{itemize}
  \item We have a theorem that says, given an instrument and monotonicity, for any $G(y)$,
    \begin{multline*}
      E[G(Y_1)\mid x, U_d=p] = \\ \E \left[ G(Y) | x, \mu_d(Z) = p, D = 1 \right] + p \frac{\partial}{\partial p} \E\left[ G(Y) |  x, \mu_d(Z) = p, D = 1 \right]
    \end{multline*}
  \item Suggests different $G(.)$ may be suited to different estimators
    \item They will impose additional assumptions to make estimation feasible
  \end{itemize}
    $\rightarrow$ Estimation is not obvious, and depends on the empirical setting
  
  
\end{frame}

\begin{frame}{Estimation}
  Two objects of interest :
\begin{enumerate}
\item Marginal potential outcomes
\begin{equation*}
  \mathrm{E}[Y_1\mid x,U_d=p]
\end{equation*}
\item Distribution of marginal potential outcomes
  \begin{equation*}
    f_{Y_1\mid X,U_d}(y\mid x,U_d=p)
  \end{equation*}
\end{enumerate}
\begin{itemize}
\item The second \textit{requires} the first.
\end{itemize}

\end{frame}


\begin{frame}{Estimation: Marginal Potential Outcomes  }
  \begin{itemize}
  \item Authors assume linearity and separability for $\mu_1$:
    \begin{equation*}
      Y_1  = X'\beta_1 +  U_1
    \end{equation*}
\item So we wish to estimate:
    \begin{equation*}
      \mathrm{E}[Y_1\mid x,U_d=p]  = x'\beta_1 +  \mathrm{E}[U_1\mid x,U_d=p]
    \end{equation*}
\item The two terms are estimated separately - let's look at each in turn
  \end{itemize}
\end{frame}

\begin{frame}{Estimation:  Marginal Potential Outcomes, $\beta_1$}
  \begin{itemize}
\item Take a different conditional expectation:
\begin{equation*}
  \mathrm{E}[Y_1\mid x, \mu_d=p,D=1] = x'\beta_1 +  \mathrm{E}[U_1\mid x,\mu_d=p,
  D=1   ]
\end{equation*}
\item It can be shown $\mathrm{E}[U_1\mid x,\mu_d=p,
  D=1   ]$ depends only on $p$. Say:
  \begin{equation*}
    \mathrm{E}[Y_1\mid x, \mu_d=p,D=1] = x'\beta_1 +  \lambda_1(p)
  \end{equation*}
%%%[The idea is that (z,x) drop out by independence, then Ud drops out
%%%because its uniform.][Also ind of x not necessary]
\item If define mean zero errors $\epsilon_i$, this can be written as:
 \begin{equation*}
    Y_1 = x'\beta_1 +  \lambda_1(p) + \epsilon
  \end{equation*}
%%%[Y=Y1 since we are among D=1]
\item This is a `Partial Linear Model'!
\end{itemize}
\end{frame}

\begin{frame}
  \begin{align*}
  \mathrm{E}[U_1\mid x,\mu_d=p,
  D=1   ] = \mathrm{E}[U_1\mid x, \mu_d=p, U_d < \mu_d    ] \\ = \mathrm{E}[U_1\mid  U_d < p    ] = \frac{1}{Pr(U_d<p)} \int_0^pE[U_1\mid \mu_d=s] ds
\end{align*}
\end{frame}

\begin{frame}{Estimation:  Marginal Potential Outcomes, $\beta_1$}
  \begin{itemize}
  \item Robinson (1988) has an estimator for this model
\item The idea is to
  `difference' away $\lambda(p)$
%%%like in panel data:
  \begin{equation*}
Y_1 - E[Y_1\mid \mu_d=p] = (x'-E[x'\mid \mu_d=p])\beta_1 +  \cancel{\lambda_1(p) - \lambda_1(p)} + \eta
\end{equation*}
\item The estimation involves:
\begin{enumerate}
\item Estimate ${p}$ non-parametrically
%%%[use siepe regression: a power series approximation]
\item Regress $Y$ on $\hat{p}$ and $X$ on $\hat{p}$, store the fitted values
\item Then do OLS of $Y - E[Y\mid \hat{p}]$ on $X -
   E[X\mid \hat{p}]$
\end{enumerate}
  \end{itemize}

\end{frame}

\begin{frame}{Estimation:  Marginal Potential Outcomes }
  \begin{itemize}
  \item Now for the second bit: \begin{equation*}
      \mathrm{E}[Y_1\mid x,U_d=p]  = x'\beta_1 +  \mathbf{\mathrm{\mathbf{E}}[U_1\mid x,U_d=p]}
    \end{equation*}
\item Our sample is $\{\hat{U}_1= Y-\mu_1(x,\hat{\beta}_1), \hat{P},
  S\}$
\item The ID theorem applies for $G(Y_1)=U_1$, so
\begin{equation*}
  \mathrm{E}[U_1\mid U_d= p] = p \frac{\partial}{\partial
    p}\mathrm{E}[U_1\mid P=p,D=1] + \mathrm{E}[U_1\mid P=p,D=1]
\end{equation*}
\item We can estimate the RHS with local polynomial regression
  \end{itemize}

\end{frame}

\begin{frame}{Estimation:  Marginal Potential Outcomes,
    $\mathrm{E}[U_1\mid U_d=p]$}
    \begin{itemize}
  \item LP regression is like LS weighting obserations close to $\hat{P}=p$
%%% [``locally'']
\item For example, solve
  \begin{equation*}
   \min_{\gamma_1,\gamma_2} \left\{ \sum_i\left[\hat{U}_{1i}-(\gamma_1 +
      \gamma_2(\hat{P}_i-p))\right]^2\mathrm{1}{\bigl(|\hat{P}_i - p| \le h\bigr)}\right\}
  \end{equation*}
%%% [I've told you what it is, now I'll give you motivation]
\item Why? Think of a taylor series of $E[\hat{U}_{1}|
 \hat{P}=\hat{P}_i]$
  about $\hat{P} = p$
  \begin{equation*}
    E[\hat{U}_{1}\mid \hat{P}_i] \approx E[\hat{U}_{1}\mid p] +
    \frac{\partial}{\partial p} E[\hat{U}_{1}\mid p] (\hat{P}_i-p)
  \end{equation*}
With equality as $p \rightarrow \hat{P}_i$. This justifies
interpretation that
\begin{eqnarray*}
  \hat{\gamma}_1 = \hat{E}[\hat{U}_{1}\mid p] & & \hat{\gamma}_2 = \frac{\partial}{\partial p} \hat{E}[\hat{U}_{1}\mid p]
\end{eqnarray*}
  \end{itemize}
\end{frame}

\begin{frame}{Recap:  Marginal Potential Outcomes}
\begin{enumerate}
\item Marginal potential outcomes---difference is MTE
\begin{equation*}
  \mathrm{E}[Y_1\mid x,U_d=p] = x'\beta_1 +
  \mathrm{E}[U_1\mid x,U_d=p]
\end{equation*}
\item Distribution of marginal potential outcomes
  \begin{equation*}
    f_{Y_1\mid X,U_d}(y\mid x,p)
  \end{equation*}
\end{enumerate}

We did \#1 by
\begin{itemize}
\item Imposing linearity and separability, and independence of $X$
\item Recovering $\hat{P}$ using sieves
\item Recovering $\hat{\beta}$ using Robinson's partial linear
  estimator
\item Estimating the selection error by local polynomial regression
\end{itemize}
Now \#2...

\end{frame}

\begin{frame}{Estimation: $f_{Y_1\mid X,U_d}(y\mid x,p)$}
  \begin{itemize}
\item Authors assume $X$ is independent of $U_1,U_d$, so
\begin{equation*}
  f_{Y_1|x,\mu_d=p}(y) = f_{U_1|\mu_d=p}(y-\mu_1(x,\beta_1))
\end{equation*}
%%% I could write this on the board if unclear

\item From ID theorem:
  \begin{equation*}
    f_{u_1|p}(u) = p \frac{\partial}{\partial
    p}f_{u_.|p,s=1}(u) + f_{u_.|p,s=1}(u)
  \end{equation*}
%%% If discrete use indicator, if cont use CDF and differentiate
\item There is also a result that says kernel density estimation is
  unbiased:
  \begin{equation*}
    \mathrm{E}\left[h^{-1}K\left(\frac{U_1-u}{h}\right)\mid P=p,
      D=1\right]\xrightarrow[h \rightarrow 0]{} f_{U_1|P,D=1}(u\mid p)
  \end{equation*}
%%% Says that
\item So suggests to get density, we could estimate the conditional mean of
  $h^{-1}K(.)$ locally (among those with $P=p$) among the treated
  \end{itemize}
 $\rightarrow$ This is local polynomial regression again


\end{frame}



\end{document}
